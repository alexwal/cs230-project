{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use TF 2.0 (on EC2 instance running Deep Learning AMI):\n",
    "# source activate tensorflow_p36\n",
    "# pip uninstall tensorflow-gpu\n",
    "# pip install tensorflow-gpu==2.0.0-alpha0\n",
    "\n",
    "# But then later decided wanted even newer GPU stuff and \"in that conda env, ran:\n",
    "# pip uninstall tensorflow-gpu\n",
    "# pip install --upgrade pip\n",
    "# pip install wrapt --ignore-installed # ran this because had an error\n",
    "# pip install  tf-nightly-gpu-2.0-preview\n",
    "\n",
    "# Result: Successfully installed tf-nightly-gpu-2.0-preview-2.0.0.dev20190601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0-dev20190601\n"
     ]
    }
   ],
   "source": [
    "from packaging import version\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__) # make sure >= 2.0.0-dev20190601\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \"This notebook requires TensorFlow 2.0 or above.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs230_project_utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU usage logging (TF 2.0+)\n",
    "\n",
    "tf.config.set_soft_device_placement(True)\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... Shuffle items? True. Shuffle buffer: 1\n",
      "Loading dataset... Shuffle items? True. Shuffle buffer: 1\n",
      "Loading dataset... Shuffle items? True. Shuffle buffer: 1\n"
     ]
    }
   ],
   "source": [
    "# Use tf.data.Datasets to preprocess and iterate data efficiently\n",
    "\n",
    "# include class_0?\n",
    "test_data_locations = dev_data_locations = train_data_locations = ['fastmri-test-tfrecords/*.tfrecord']\n",
    "\n",
    "# Create preprocessing functions\n",
    "\n",
    "preprocessing_fn = utils.fastmri.fastMRIPreprocessor(shape=(128, 128),\n",
    "                                                     use_tiled_reflections=False,\n",
    "                                                     subsampling_mask_function=None,\n",
    "                                                     normalize=True)\n",
    "\n",
    "batch_size = 4\n",
    "shuffle_buffer_size = 1\n",
    "\n",
    "# Load data\n",
    "\n",
    "test_dataset = utils.fastmri.load_dataset(test_data_locations,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                     include_all_parsed_features=False,\n",
    "                                     ignore_errors=True,\n",
    "                                     preprocessing_function=preprocessing_fn)\n",
    "\n",
    "dev_dataset = utils.fastmri.load_dataset(dev_data_locations,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                     include_all_parsed_features=True,\n",
    "                                     ignore_errors=True,\n",
    "                                     preprocessing_function=preprocessing_fn)\n",
    "\n",
    "train_dataset = utils.fastmri.load_dataset(train_data_locations,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle_buffer_size=shuffle_buffer_size,\n",
    "                                     include_all_parsed_features=False,\n",
    "                                     ignore_errors=True,\n",
    "                                     preprocessing_function=preprocessing_fn)\n",
    "\n",
    "\n",
    "# Debugging: make these datasets tiny\n",
    "\n",
    "test_dataset = test_dataset.take(3)\n",
    "dev_dataset = dev_dataset.take(3)\n",
    "train_dataset = train_dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First batch of each dataset to be used in plotting images periodically to tensorboard\n",
    "\n",
    "first_test_batch = next(iter(test_dataset))\n",
    "first_dev_batch = next(iter(dev_dataset))\n",
    "first_train_batch = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A metric to use during training\n",
    "def mean_PSNR(y_true, y_pred):\n",
    "    max_value = 1.0\n",
    "    MSE = tf.reduce_mean(tf.square(y_true - y_pred), axis=[1, 2, 3])\n",
    "    PSNR = 10 * tf.math.log(tf.divide(max_value ** 2, MSE)) / tf.math.log(tf.constant(10, dtype=y_pred.dtype))\n",
    "    mean = tf.reduce_mean(PSNR)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uncompiled_automap_model():\n",
    "\n",
    "    N = 128\n",
    "    X = tf.keras.layers.Input(shape=(N, N, 2))\n",
    "\n",
    "    # Paper says 1% multiplicative gaussian noise (this multiplies by 1-centered gaussian\n",
    "    # having stddev = sqrt(rate / (1 - rate)) (here, 0.00032...)\n",
    "    # noisy_X = tf.keras.layers.GaussianDropout(rate=1e-7)(X) # spatial dimension: 256\n",
    "    # Note: (we could corrupt when training with cifar, but maybe not other dataset?)\n",
    "\n",
    "    ds_flat = tf.keras.layers.Flatten()(X)\n",
    "\n",
    "    fc1 = tf.keras.layers.Dense(64 ** 2, activation='tanh')(ds_flat) # closer to original paper is: 128 ** 2\n",
    "    fc1 = tf.keras.layers.Dropout(0.05)(fc1)\n",
    "    fc2 = tf.keras.layers.Dense(128 ** 2, activation='tanh')(fc1)\n",
    "\n",
    "    fc_output = tf.keras.layers.Reshape([128, 128, 1])(fc2)\n",
    "\n",
    "    conv1 = tf.keras.layers.Conv2D(64, (5, 5), strides=(1, 1), activation='relu', padding='same')(fc_output)\n",
    "    \n",
    "    # L1 regularization to encourage sparsity\n",
    "    conv2 = tf.keras.layers.Conv2D(64, (5, 5), strides=(1, 1), activation='relu', padding='same',\n",
    "                                   activity_regularizer=tf.keras.regularizers.l1(1e-4))(conv1)\n",
    "\n",
    "    Y_pred = tf.keras.layers.Conv2DTranspose(1, (7, 7), strides=1, activation='relu', padding='same')(conv2) # upsample to spatial dimension 256\n",
    "\n",
    "    model = tf.keras.Model(inputs=X, outputs=Y_pred)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compiled_automap_model():\n",
    "    multi_gpu = False\n",
    "    # Distribute training across GPUs (each GPU receives identical updates to weights but different batches w/\n",
    "    # mirrored strategy). Restricts callbacks we can use\n",
    "    if multi_gpu:\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            model = load_uncompiled_automap_model()\n",
    "            model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(), metrics=[mean_PSNR])\n",
    "    else:\n",
    "        model = load_uncompiled_automap_model()\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(), metrics=[mean_PSNR])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_compiled_automap_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 2)]     0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              134221824 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16384)             67125248  \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 64)      1664      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 128, 128, 1)       3137      \n",
      "=================================================================\n",
      "Total params: 201,454,337\n",
      "Trainable params: 201,454,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate schedule\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"\n",
    "    Returns a custom learning rate that decreases as epochs progress.\n",
    "    \"\"\"\n",
    "    if 0 <= epoch < 100:\n",
    "        learning_rate = 1e-4\n",
    "    elif 100 <= epoch < 200:\n",
    "        learning_rate = 5e-5\n",
    "    elif 200 <= epoch < 400:\n",
    "        learning_rate = 2.55e-5\n",
    "    elif 400 <= epoch < 600:\n",
    "        learning_rate = 1e-5\n",
    "    elif 600 <= epoch < 700:\n",
    "        learning_rate = 2.5e-5\n",
    "    elif 700 <= epoch < 800:\n",
    "        learning_rate = 1e-5\n",
    "    elif 800 <= epoch < 900:\n",
    "        learning_rate = 1e-6\n",
    "    else:\n",
    "        learning_rate = 1e-7\n",
    "\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "        \n",
    "    return learning_rate\n",
    "\n",
    "# Show reconstructions during training\n",
    "\n",
    "def plot_fft_reconstructions(batch, logs):\n",
    "    plot_frequency = 2\n",
    "    \n",
    "    if batch % plot_frequency != 0:\n",
    "        return\n",
    "    \n",
    "    batches = [(first_test_batch, 'Test')]#, (first_dev_batch, 'Dev'), (first_train_batch, 'Train')]\n",
    "    \n",
    "    for dataset_batch, name in batches:\n",
    "        x, y = dataset_batch\n",
    "        y = y.numpy()\n",
    "        y_pred = model.predict(x)\n",
    "\n",
    "        with file_writer.as_default():\n",
    "            for i in range(min(len(y), 8)):\n",
    "                prediction, ground_truth = y_pred[i:i + 1, ...], y[i:i + 1, ...]\n",
    "                tf.summary.image(\"{} Image {} (Prediction)\".format(name, i), prediction, max_outputs=1, step=batch)\n",
    "                tf.summary.image(\"{} Image {} (Ground Truth)\".format(name, i), ground_truth, max_outputs=1, step=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear logs if necessary\n",
    "# !rm -r logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where logs will be saved\n",
    "\n",
    "logdir = os.path.join('logs', datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
    "file_writer = tf.summary.create_file_writer(os.path.join(logdir, 'metrics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks to use in various stages of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "\n",
    "class PrintAndLogLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.scalar('learning rate (end of epoch)', data=model.optimizer.lr.numpy(), step=epoch)\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                          model.optimizer.lr.numpy()))\n",
    "\n",
    "plot_images_callback = tf.keras.callbacks.LambdaCallback(on_batch_end=plot_fft_reconstructions)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, update_freq=1,\n",
    "                                                      profile_batch=0) # workaround for: https://github.com/tensorflow/tensorboard/issues/2084\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.25,\n",
    "                              patience=2, min_lr=1e-8)\n",
    "\n",
    "callbacks = [tensorboard_callback, lr_callback, plot_images_callback, reduce_lr_callback, PrintAndLogLR()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin training and open Tensorboard to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "      3/Unknown - 14s 5s/step - loss: 0.5961 - mean_PSNR: 10.6846\n",
      "Learning rate for epoch 1 is 9.999999747378752e-05\n",
      "3/3 [==============================] - 32s 11s/step - loss: 0.5961 - mean_PSNR: 10.6846 - val_loss: 0.0000e+00 - val_mean_PSNR: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/3 [===================>..........] - ETA: 4s - loss: 0.2651 - mean_PSNR: 10.8247"
     ]
    }
   ],
   "source": [
    "training_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1, # set to 0 to suppress chatty output and use Tensorboard instead\n",
    "    epochs=2,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=True) # see if speeds things up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model (to do: make callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Uncomment to save model\n",
    "# saved_model_path = 'automap_our_dataset_original_paper_model_with_up_down_sampling_single_GPU_small_FC_v6'\n",
    "# model.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, verbose=1)\n",
    "# model.evaluate(dev_dataset, verbose=1)\n",
    "# model.evaluate(train_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history (but better viewed in Tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_history.history[\"loss\"], label=\"Train\")\n",
    "plt.plot(training_history.history[\"val_loss\"], label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"center right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(training_history.history[\"mean_PSNR\"], label=\"Train\")\n",
    "plt.plot(training_history.history[\"val_mean_PSNR\"], label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"µ ( PSNR ) \")\n",
    "plt.legend(loc=\"center right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a test batch\n",
    "\n",
    "# batch = next(iter(test_dataset))\n",
    "\n",
    "x, y = first_test_batch\n",
    "x, y = x.numpy(), y.numpy()\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# Inspect output\n",
    "\n",
    "for i in range(len(x)):\n",
    "\n",
    "    fft_mag = x[i, ..., 0]\n",
    "    fft_ang = x[i, ..., 1]\n",
    "    reconstruction = y_pred[i, ..., 0]\n",
    "    image = y[i, ..., 0]\n",
    "\n",
    "    MSE = utils.signal_processing.mean_square_error(reconstruction, image)\n",
    "    PSNR = utils.signal_processing.PSNR(reconstruction, image, max_value=max(image.max(), reconstruction.max()))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.title('Reconstruction (MSE: {:0.5f}, PSNR: {:0.5f})'.format(MSE, PSNR))\n",
    "    utils.plot.imshowgray(reconstruction)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.title('FFT (Magnitude)')\n",
    "    utils.plot.imshowfft(fft_mag)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.title('Expected reconstruction')\n",
    "    utils.plot.imshowgray(image)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title('FFT (Phase)')\n",
    "    utils.plot.imshowgray(fft_ang)\n",
    "\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_pred[0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
